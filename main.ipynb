{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c5b7c-45b4-4b79-859f-dc0eb185d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from acai.autoencoders.base import AutoencoderBase\n",
    "from acai.backbones import build_simple_encoder, build_simple_decoder\n",
    "from acai.data.line_dataloader import LineDataLoader\n",
    "from acai.data.mnist_dataloader import MNISTDataLoader\n",
    "from acai.image_utils import torch2numpy_image, draw_images, collage_images\n",
    "from acai.wandb_logger import WandbLogger\n",
    "from acai.utils import init_weights_kaiming_normal, fix_seeds\n",
    "from acai.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb186e6-81f5-4f26-b98a-c035edc27896",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05ab845",
   "metadata": {},
   "source": [
    "# Quick look into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018be30-164e-42fe-b945-abdf4a5f7d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.data.line_image_generation import make_line_image\n",
    "fix_seeds(0)\n",
    "angles = np.random.uniform(-3, 3, size=10)\n",
    "images = np.stack([make_line_image(angle) for angle in tqdm.tqdm(angles)])\n",
    "images = images / 255\n",
    "images = images.astype(np.float32)\n",
    "draw_images(images[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# dataloader = LineDataLoader(batch_size=64, device=device)\n",
    "dataloader = MNISTDataLoader(batch_size=64, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4362e47-9ef7-44b7-886c-0e654a4fd21b",
   "metadata": {},
   "source": [
    "## Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffaba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderTrainer(Trainer):\n",
    "    def _configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(self.autoencoder.parameters(), lr=3e-4)\n",
    "\n",
    "    def _train_step(self):\n",
    "        self.autoencoder.train()\n",
    "        batch = self.dataloader.get_train_batch()\n",
    "        first_image = torch2numpy_image(batch[0])\n",
    "        out = self.autoencoder(batch)\n",
    "        loss = out['loss']\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.logger.log({\"ae/train/loss\": loss.detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"ae/train/images/first_image_in_batch_reconstruction\", \n",
    "            collage_images([\n",
    "                first_image, \n",
    "                torch2numpy_image(out['reconstructed_images'][0]),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    def _eval_step(self, batch):\n",
    "        self.autoencoder.eval()\n",
    "        out = self.autoencoder(batch)\n",
    "        loss = out['loss']\n",
    "        self.logger.log({\"ae/test/loss\": loss.detach().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad25d2b-38be-412c-bd1b-83ab65fac187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders.autoencoder import AutoencoderDefault\n",
    "fix_seeds(42)\n",
    "\n",
    "logger = WandbLogger(run='ae_mnist', project='acai_karpathy', log_images_every=100)\n",
    "encoder = build_simple_encoder(width_coef=1)\n",
    "decoder = build_simple_decoder(width_coef=1)\n",
    "autoencoder = AutoencoderDefault(encoder, decoder)\n",
    "autoencoder.apply(init_weights_kaiming_normal)\n",
    "\n",
    "trainer = AutoencoderTrainer(autoencoder, dataloader, logger=logger, device=torch.device(\"cuda\"))\n",
    "trainer.train(n_steps=260000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855394b5-a7ff-4ba6-a02a-5276f4e4813b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train ACAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0822dc7-8dd6-4171-bbdd-610f6e539d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders.acai import ACAI\n",
    "from acai.monitoring.interpolation import InterpolationMonitoring\n",
    "\n",
    "\n",
    "class ACAITrainer(Trainer):\n",
    "    def _configure_optimizers(self):\n",
    "        self.optimizer_ae = torch.optim.Adam(\n",
    "            [\n",
    "                *self.autoencoder.encoder.parameters(), \n",
    "                *self.autoencoder.decoder.parameters(),\n",
    "            ], \n",
    "            lr=3e-4\n",
    "        )\n",
    "        self.optimizer_critic = torch.optim.Adam(self.autoencoder.critic.parameters(), lr=3e-4)\n",
    "    \n",
    "    def _train_step(self):\n",
    "        self._train_ae()\n",
    "        self._train_critic()\n",
    "    \n",
    "    def _train_ae(self):\n",
    "        batch = self.dataloader.get_train_batch()\n",
    "        batch4interpolation = self.dataloader.get_train_batch()\n",
    "        first_image = torch2numpy_image(batch[0])\n",
    "        out = self.autoencoder(batch, batch4interpolation)\n",
    "        self.optimizer_ae.zero_grad()\n",
    "        out['loss'].backward()\n",
    "        self.optimizer_ae.step()\n",
    "\n",
    "        self.logger.log({\"ae/train/loss\": out['loss'].detach().item()})\n",
    "        self.logger.log({\"ae/train/reconstruction_loss\": out['reconstruction_loss'].detach().item()})\n",
    "        self.logger.log({\"ae/train/interpolation_loss\": out['interpolation_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"ae/train/images/first_image_in_batch_reconstruction\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"ae/train/images/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        \n",
    "    def _train_critic(self):\n",
    "        batch = self.dataloader.get_train_batch()\n",
    "        batch4interpolation = self.dataloader.get_train_batch()\n",
    "        first_image = torch2numpy_image(batch[0])\n",
    "        out = self.autoencoder.forward_critic(batch, batch4interpolation)\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        out['loss'].backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        self.logger.log({\"critic/train/loss\": out['loss'].detach().item()})\n",
    "        self.logger.log({\"critic/train/alpha_recovery_loss\": out['alpha_recovery_loss'].detach().item()})\n",
    "        self.logger.log({\"critic/train/regularization_loss\": out['regularization_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"critic/train/images/first_image_in_batch\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"critic/train/images/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"critic/train/images/blended_input_image\",\n",
    "            torch2numpy_image(out[\"blended_non_interpolated_images\"][0]),\n",
    "        )\n",
    "\n",
    "    def _eval_step(self, batch):\n",
    "        self.autoencoder.eval()\n",
    "        batch4interpolation = self.dataloader.get_train_batch()\n",
    "        out = self.autoencoder(batch, batch4interpolation)\n",
    "        self.logger.log({\"ae/test/loss\": out['loss'].detach().item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630b888-9a1f-44f4-8ed5-83fc1ef030e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(42)\n",
    "\n",
    "logger = WandbLogger(run='acai_mnist', project='acai_karpathy', log_images_every=100)\n",
    "encoder = build_simple_encoder(width_coef=1)\n",
    "decoder = build_simple_decoder(width_coef=1)\n",
    "critic = build_simple_encoder(width_coef=1)\n",
    "autoencoder = ACAI(encoder, decoder, critic)\n",
    "autoencoder.apply(init_weights_kaiming_normal)\n",
    "\n",
    "trainer = ACAITrainer(autoencoder, dataloader, logger=logger, device=torch.device(\"cuda\"))\n",
    "trainer.train(n_steps=260000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
