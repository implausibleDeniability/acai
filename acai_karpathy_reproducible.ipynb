{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391c5b7c-45b4-4b79-859f-dc0eb185d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from acai.autoencoders import AutoencoderBase\n",
    "from acai.backbones import build_simple_encoder, build_simple_decoder\n",
    "from acai.convolution_visualizer import ConvolutionVisualizer\n",
    "from acai.data import make_line_image\n",
    "from acai.image_utils import collage_images, torch2numpy_image, unravel_images, draw_images\n",
    "from acai.wandb_logger import WandbLogger\n",
    "from acai.utils import init_weights_kaiming_normal, fix_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb186e6-81f5-4f26-b98a-c035edc27896",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCHSIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7018be30-164e-42fe-b945-abdf4a5f7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 38164.73it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix_seeds(0)\n",
    "angles = np.random.uniform(-3, 3, size=10)\n",
    "images = np.stack([make_line_image(angle) for angle in tqdm.tqdm(angles)])\n",
    "images = images / 255\n",
    "images = images.astype(np.float32)\n",
    "draw_images(images[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddba699-b143-4568-9ec7-8462a7f6c2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae88c517-6583-4c12-bb5c-53429e7222b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 3.1416\n",
    "\n",
    "def generate_batch(batchsize: int) -> torch.Tensor:\n",
    "    angles = np.random.uniform(-PI, PI, size=batchsize)\n",
    "    images = [make_line_image(angle) for angle in angles]\n",
    "    batch = np.stack(images)\n",
    "    batch = batch / 255\n",
    "    batch = torch.FloatTensor(batch)\n",
    "    batch = batch.view(batchsize, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d933848-0f80-4c92-a0f9-ee55f48a9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Trainer:\n",
    "    N_EVAL_LOGGING_IMAGES = 3\n",
    "    EVAL_EVERY = 500\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: AutoencoderBase,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        self.autoencoder = autoencoder\n",
    "        self.logger = logger or WandbLogger('debug', 'acai_karpathy')\n",
    "        self.device = device\n",
    "        self.convolution_visualizer = ConvolutionVisualizer()\n",
    "        \n",
    "    def train(self, n_steps):\n",
    "        eval_batch = generate_batch(batchsize=64).to(self.device)\n",
    "        self.autoencoder = self.autoencoder.to(self.device)\n",
    "        for step in range(n_steps):\n",
    "            self.train_step()\n",
    "            if step % self.EVAL_EVERY == 0:\n",
    "                self.eval_step(eval_batch)\n",
    "                # self.monitor_step()\n",
    "            self.logger.commit()\n",
    "\n",
    "    def _train_step(self) -> None:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        self.autoencoder.eval()\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_images = torch2numpy_image(batch[:self.N_EVAL_LOGGING_IMAGES])\n",
    "        out = self.autoencoder(batch, batch2)\n",
    "        loss = out['loss']\n",
    "        reconstructed_first_images = torch2numpy_image(out['reconstructed_images'][:self.N_EVAL_LOGGING_IMAGES])\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/eval/reconstruction\", \n",
    "            collage_images([first_images, reconstructed_first_images])\n",
    "        )\n",
    "        self.logger.log({\"ae/test/loss\": loss.detach().item()})\n",
    "        \n",
    "    def monitor_step(self):\n",
    "        convolutions: dict[str, np.ndarray] = self.convolution_visualizer.extract_convolutions(self.autoencoder)\n",
    "        for name, weights in convolutions.items():\n",
    "            unravelled_images = unravel_images(weights, max_rows=8)\n",
    "            weights_collage = collage_images(unravelled_images)\n",
    "            self.logger.log_images(\n",
    "                \"images/ae/convolutions/\" + name,\n",
    "                weights_collage,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4362e47-9ef7-44b7-886c-0e654a4fd21b",
   "metadata": {},
   "source": [
    "## Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffaba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: AutoencoderBase,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__(autoencoder, logger, device)\n",
    "        self.compute_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def train_step(self):\n",
    "        self.autoencoder.train()\n",
    "        batch = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch[0])\n",
    "        out = self.autoencoder(batch)\n",
    "        loss = out['loss']\n",
    "        reconstructed_first_image = torch2numpy_image(out['reconstructed_images'][0])\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/first_image_in_batch\", \n",
    "            collage_images([first_image, reconstructed_first_image])\n",
    "        )\n",
    "        self.logger.log({\"ae/train/loss\": loss.detach().item()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad25d2b-38be-412c-bd1b-83ab65fac187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders import AutoencoderDefault\n",
    "# fix_seeds(42)\n",
    "\n",
    "# logger = WandbLogger(run='aekn8bbs', project='acai_karpathy', log_images_every=100)\n",
    "# encoder = build_simple_encoder(width_coef=1)\n",
    "# decoder = build_simple_decoder(width_coef=1)\n",
    "# autoencoder = AutoencoderDefault(encoder, decoder)\n",
    "# autoencoder.apply(init_weights)\n",
    "\n",
    "# trainer = Trainer(autoencoder, logger=logger, device=torch.device(\"cuda\"))\n",
    "# trainer.train(n_steps=260000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855394b5-a7ff-4ba6-a02a-5276f4e4813b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train ACAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b66af7bc-2851-4f85-859d-c8bd455a91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders import AutoencoderBase\n",
    "from typing import Any\n",
    "\n",
    "class ACAI(AutoencoderBase):\n",
    "    GAMMA = 0.2\n",
    "    LAMBDA = 0.5 / 4\n",
    "    \n",
    "    def __init__(self, encoder, decoder, critic):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.critic = critic\n",
    "        self.compute_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, images: torch.Tensor, images4interpolation: torch.Tensor) -> dict[str, Any]:\n",
    "        latent = self.encoder(images)\n",
    "        latent4interpolation = self.encoder(images4interpolation)\n",
    "        interpolated_latent, alphas = self._mix_latents(latent, latent4interpolation)\n",
    "        reconstructed_images = self.decoder(latent)\n",
    "        interpolated_images = self.decoder(interpolated_latent)\n",
    "        predicted_alphas = torch.mean(self.critic(interpolated_images), [1, 2, 3])\n",
    "        \n",
    "        reconstruction_loss = self.compute_loss(reconstructed_images, images)\n",
    "        interpolation_loss = self.LAMBDA * (predicted_alphas ** 2).mean(axis=0)\n",
    "        loss = reconstruction_loss + interpolation_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"interpolation_loss\": interpolation_loss,\n",
    "            \"reconstructed_images\": reconstructed_images,\n",
    "            \"reconstructed_interpolated_images\": interpolated_images,\n",
    "            \"reconstructed_interpolated_alphas\": predicted_alphas,\n",
    "        }\n",
    "    \n",
    "    def _mix_latents(self, latent, latent4interpolation) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param latent: torch.Tensor of shape [BATCH_SIZE, CHANNELS, HEIGHT, WIDTH]\n",
    "        :return: tuple of two tensors. \n",
    "            1. Mixed latents, torch.Tensor of shape [BATCH_SIZE, CHANNELS, HEGIHT, WIDTH]\n",
    "            2. Alphas, torch.Tensor of shape [BATCH_SIZE, 1, 1, 1]\n",
    "        \"\"\"\n",
    "        batch_size = latent.shape[0]\n",
    "        alphas = 0.5 * torch.rand(batch_size).reshape(-1, 1, 1, 1).to(latent.device)\n",
    "        interpolated_latent = alphas * latent + (1 - alphas) * latent4interpolation\n",
    "        return interpolated_latent, alphas\n",
    "    \n",
    "    def forward_critic(self, images: torch.Tensor, images4interpolation: torch.Tensor) -> dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            latent = self.encoder(images)\n",
    "            latent4interpolation = self.encoder(images4interpolation)\n",
    "            interpolated_latent, alphas = self._mix_latents(latent, latent4interpolation)\n",
    "            reconstructed_images = self.decoder(latent)\n",
    "            interpolated_images = self.decoder(interpolated_latent)\n",
    "            regularization_images = self.GAMMA * images + (1 - self.GAMMA) * reconstructed_images\n",
    "        predicted_alphas = torch.mean(self.critic(interpolated_images), [1, 2, 3])\n",
    "        predicted_alphas_regularization = torch.mean(self.critic(regularization_images), [1, 2, 3])\n",
    "\n",
    "        alpha_recovery_loss = torch.nn.functional.mse_loss(predicted_alphas, alphas.squeeze())\n",
    "        regularization_loss = (predicted_alphas_regularization ** 2).mean(0)\n",
    "        loss = alpha_recovery_loss + regularization_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"alpha_recovery_loss\": alpha_recovery_loss,\n",
    "            \"regularization_loss\": regularization_loss,\n",
    "            \"reconstructed_interpolated_images\": interpolated_images,\n",
    "            \"reconstructed_images\": reconstructed_images,\n",
    "            \"blended_non_interpolated_images\": regularization_images,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0822dc7-8dd6-4171-bbdd-610f6e539d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAITrainer(Trainer):\n",
    "    autoencoder: ACAI\n",
    "    TRAIN_CRITIC_AFTER = 0\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: ACAI,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__(autoencoder, logger, device)\n",
    "        self.optimizer_ae = torch.optim.Adam([*autoencoder.encoder.parameters(), *autoencoder.decoder.parameters()], lr=3e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(autoencoder.critic.parameters(), lr=3e-4)\n",
    "        self._step_counter = 0\n",
    "    \n",
    "    def train_step(self):\n",
    "        self.autoencoder.train()\n",
    "        self._train_ae()\n",
    "        if self.TRAIN_CRITIC_AFTER < self._step_counter:\n",
    "            self._train_critic()\n",
    "        self._step_counter += 1\n",
    "    \n",
    "    def _train_ae(self):\n",
    "        # self.autoencoder.train()\n",
    "        # self.autoencoder.critic.eval()\n",
    "        batch1 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch1[0])\n",
    "        out = self.autoencoder(batch1, batch2)\n",
    "        loss = out['loss']\n",
    "        self.optimizer_ae.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_ae.step()\n",
    "\n",
    "        self.logger.log({\"ae/train/loss\": loss.detach().item()})\n",
    "        self.logger.log({\"ae/train/reconstruction_loss\": out['reconstruction_loss'].detach().item()})\n",
    "        self.logger.log({\"ae/train/interpolation_loss\": out['interpolation_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/first_image_in_batch\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        \n",
    "    def _train_critic(self):\n",
    "        # self.autoencoder.eval()\n",
    "        # self.autoencoder.critic.train()\n",
    "        batch1 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch1[0])\n",
    "        out = self.autoencoder.forward_critic(batch1, batch2)\n",
    "        loss = out['loss']\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        self.logger.log({\"critic/train/loss\": loss.detach().item()})\n",
    "        self.logger.log({\"critic/train/alpha_recovery_loss\": out['alpha_recovery_loss'].detach().item()})\n",
    "        self.logger.log({\"critic/train/regularization_loss\": out['regularization_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/first_image_in_batch\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/blended_image\",\n",
    "            torch2numpy_image(out[\"blended_non_interpolated_images\"][0]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630b888-9a1f-44f4-8ed5-83fc1ef030e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimplausible_deniability\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maxim/acai/wandb/run-20231025_112247-ciwc1f6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/implausible_deniability/acai_karpathy/runs/ciwc1f6j' target=\"_blank\">acaild4</a></strong> to <a href='https://wandb.ai/implausible_deniability/acai_karpathy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/implausible_deniability/acai_karpathy' target=\"_blank\">https://wandb.ai/implausible_deniability/acai_karpathy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/implausible_deniability/acai_karpathy/runs/ciwc1f6j' target=\"_blank\">https://wandb.ai/implausible_deniability/acai_karpathy/runs/ciwc1f6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix_seeds(42)\n",
    "\n",
    "logger = WandbLogger(run='acaild4', project='acai_karpathy', log_images_every=100)\n",
    "encoder = build_simple_encoder(width_coef=1)\n",
    "decoder = build_simple_decoder(width_coef=1)\n",
    "critic = build_simple_encoder(width_coef=1)\n",
    "autoencoder = ACAI(encoder, decoder, critic)\n",
    "autoencoder.apply(init_weights_kaiming_normal)\n",
    "\n",
    "trainer = ACAITrainer(autoencoder, logger=logger, device=torch.device(\"cuda\"))\n",
    "trainer.train(n_steps=260000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be674f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_encoder(encoder, decoder, idx=0):\n",
    "    N_STEPS = 9\n",
    "    images = generate_batch(2)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image1_latent, image2_latent = (encoder(image.unsqueeze(0)) for image in images)\n",
    "    interpolated_latents = [alpha*image1_latent + (1-alpha)*image2_latent for alpha in np.linspace(0.0, 1.0, N_STEPS)]\n",
    "    interpolated_images = [decoder(latent).squeeze() for latent in interpolated_latents]\n",
    "\n",
    "    fig, ax = plt.subplots(1, N_STEPS, figsize=(15, 5))\n",
    "    for i in range(N_STEPS):\n",
    "        ax[i].imshow(interpolated_images[i].detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a589347-d31c-43b3-a6a6-fc37cf34e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_encoder(autoencoder.encoder.cpu(), autoencoder.decoder.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa-protein",
   "language": "python",
   "name": "cafa-protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
