{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391c5b7c-45b4-4b79-859f-dc0eb185d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "from acai.autoencoders import AutoencoderBase\n",
    "from acai.backbones import build_simple_encoder, build_simple_decoder\n",
    "from acai.convolution_visualizer import ConvolutionVisualizer\n",
    "from acai.data import make_line_image\n",
    "from acai.image_utils import collage_images, torch2numpy_image, unravel_images, draw_images\n",
    "from acai.wandb_logger import WandbLogger\n",
    "from acai.utils import init_weights_kaiming_normal, fix_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb186e6-81f5-4f26-b98a-c035edc27896",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "BATCHSIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7018be30-164e-42fe-b945-abdf4a5f7d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 35365.13it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix_seeds(0)\n",
    "angles = np.random.uniform(-3, 3, size=10)\n",
    "images = np.stack([make_line_image(angle) for angle in tqdm.tqdm(angles)])\n",
    "images = images / 255\n",
    "images = images.astype(np.float32)\n",
    "draw_images(images[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddba699-b143-4568-9ec7-8462a7f6c2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae88c517-6583-4c12-bb5c-53429e7222b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = 3.1416\n",
    "\n",
    "def generate_batch(batchsize: int) -> torch.Tensor:\n",
    "    angles = np.random.uniform(-PI, PI, size=batchsize)\n",
    "    images = [make_line_image(angle) for angle in angles]\n",
    "    batch = np.stack(images)\n",
    "    batch = batch / 255\n",
    "    batch = torch.FloatTensor(batch)\n",
    "    batch = batch.view(batchsize, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d933848-0f80-4c92-a0f9-ee55f48a9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Trainer:\n",
    "    N_EVAL_LOGGING_IMAGES = 3\n",
    "    EVAL_EVERY = 500\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: AutoencoderBase,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        self.autoencoder = autoencoder\n",
    "        self.logger = logger or WandbLogger('debug', 'acai_karpathy')\n",
    "        self.device = device\n",
    "        self.convolution_visualizer = ConvolutionVisualizer()\n",
    "        \n",
    "    def train(self, n_steps):\n",
    "        eval_batch = generate_batch(batchsize=64).to(self.device)\n",
    "        self.autoencoder = self.autoencoder.to(self.device)\n",
    "        for step in range(n_steps):\n",
    "            self.train_step()\n",
    "            if step % self.EVAL_EVERY == 0:\n",
    "                self.eval_step(eval_batch)\n",
    "                # self.monitor_step()\n",
    "            self.logger.commit()\n",
    "\n",
    "    def _train_step(self) -> None:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def eval_step(self, batch):\n",
    "        self.autoencoder.eval()\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_images = torch2numpy_image(batch[:self.N_EVAL_LOGGING_IMAGES])\n",
    "        out = self.autoencoder(batch, batch2)\n",
    "        loss = out['loss']\n",
    "        reconstructed_first_images = torch2numpy_image(out['reconstructed_images'][:self.N_EVAL_LOGGING_IMAGES])\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/eval/reconstruction\", \n",
    "            collage_images([first_images, reconstructed_first_images])\n",
    "        )\n",
    "        self.logger.log({\"ae/test/loss\": loss.detach().item()})\n",
    "        \n",
    "    def monitor_step(self):\n",
    "        convolutions: dict[str, np.ndarray] = self.convolution_visualizer.extract_convolutions(self.autoencoder)\n",
    "        for name, weights in convolutions.items():\n",
    "            unravelled_images = unravel_images(weights, max_rows=8)\n",
    "            weights_collage = collage_images(unravelled_images)\n",
    "            self.logger.log_images(\n",
    "                \"images/ae/convolutions/\" + name,\n",
    "                weights_collage,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4362e47-9ef7-44b7-886c-0e654a4fd21b",
   "metadata": {},
   "source": [
    "## Train AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ffaba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: AutoencoderBase,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__(autoencoder, logger, device)\n",
    "        self.compute_loss = torch.nn.MSELoss()\n",
    "\n",
    "    def train_step(self):\n",
    "        self.autoencoder.train()\n",
    "        batch = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch[0])\n",
    "        out = self.autoencoder(batch)\n",
    "        loss = out['loss']\n",
    "        reconstructed_first_image = torch2numpy_image(out['reconstructed_images'][0])\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/first_image_in_batch\", \n",
    "            collage_images([first_image, reconstructed_first_image])\n",
    "        )\n",
    "        self.logger.log({\"ae/train/loss\": loss.detach().item()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad25d2b-38be-412c-bd1b-83ab65fac187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders import AutoencoderDefault\n",
    "# fix_seeds(42)\n",
    "\n",
    "# logger = WandbLogger(run='aekn8bbs', project='acai_karpathy', log_images_every=100)\n",
    "# encoder = build_simple_encoder(width_coef=1)\n",
    "# decoder = build_simple_decoder(width_coef=1)\n",
    "# autoencoder = AutoencoderDefault(encoder, decoder)\n",
    "# autoencoder.apply(init_weights)\n",
    "\n",
    "# trainer = Trainer(autoencoder, logger=logger, device=torch.device(\"cuda\"))\n",
    "# trainer.train(n_steps=260000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855394b5-a7ff-4ba6-a02a-5276f4e4813b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train ACAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b66af7bc-2851-4f85-859d-c8bd455a91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from acai.autoencoders import AutoencoderBase\n",
    "from typing import Any\n",
    "\n",
    "class ACAI(AutoencoderBase):\n",
    "    GAMMA = 0.2\n",
    "    LAMBDA = 0.5 / 4\n",
    "    \n",
    "    def __init__(self, encoder, decoder, critic):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.critic = critic\n",
    "        self.compute_loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, images: torch.Tensor, images4interpolation: torch.Tensor) -> dict[str, Any]:\n",
    "        latent = self.encoder(images)\n",
    "        latent4interpolation = self.encoder(images4interpolation)\n",
    "        interpolated_latent, alphas = self._mix_latents(latent, latent4interpolation)\n",
    "        reconstructed_images = self.decoder(latent)\n",
    "        interpolated_images = self.decoder(interpolated_latent)\n",
    "        predicted_alphas = torch.mean(self.critic(interpolated_images), [1, 2, 3])\n",
    "        \n",
    "        reconstruction_loss = self.compute_loss(reconstructed_images, images)\n",
    "        interpolation_loss = self.LAMBDA * (predicted_alphas ** 2).mean(axis=0)\n",
    "        loss = reconstruction_loss + interpolation_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"interpolation_loss\": interpolation_loss,\n",
    "            \"reconstructed_images\": reconstructed_images,\n",
    "            \"reconstructed_interpolated_images\": interpolated_images,\n",
    "            \"reconstructed_interpolated_alphas\": predicted_alphas,\n",
    "        }\n",
    "    \n",
    "    def _mix_latents(self, latent, latent4interpolation) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        :param latent: torch.Tensor of shape [BATCH_SIZE, CHANNELS, HEIGHT, WIDTH]\n",
    "        :return: tuple of two tensors. \n",
    "            1. Mixed latents, torch.Tensor of shape [BATCH_SIZE, CHANNELS, HEGIHT, WIDTH]\n",
    "            2. Alphas, torch.Tensor of shape [BATCH_SIZE, 1, 1, 1]\n",
    "        \"\"\"\n",
    "        batch_size = latent.shape[0]\n",
    "        alphas = 0.5 * torch.rand(batch_size).reshape(-1, 1, 1, 1).to(latent.device)\n",
    "        interpolated_latent = (1 - alphas) * latent + alphas * latent4interpolation\n",
    "        return interpolated_latent, alphas\n",
    "    \n",
    "    def forward_critic(self, images: torch.Tensor, images4interpolation: torch.Tensor) -> dict[str, Any]:\n",
    "        with torch.no_grad():\n",
    "            latent = self.encoder(images)\n",
    "            latent4interpolation = self.encoder(images4interpolation)\n",
    "            interpolated_latent, alphas = self._mix_latents(latent, latent4interpolation)\n",
    "            reconstructed_images = self.decoder(latent)\n",
    "            interpolated_images = self.decoder(interpolated_latent)\n",
    "            regularization_images = self.GAMMA * images + (1 - self.GAMMA) * reconstructed_images\n",
    "        predicted_alphas = torch.mean(self.critic(interpolated_images), [1, 2, 3])\n",
    "        predicted_alphas_regularization = torch.mean(self.critic(regularization_images), [1, 2, 3])\n",
    "\n",
    "        alpha_recovery_loss = torch.nn.functional.mse_loss(predicted_alphas, alphas.squeeze())\n",
    "        regularization_loss = (predicted_alphas_regularization ** 2).mean(0)\n",
    "        loss = alpha_recovery_loss + regularization_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"alpha_recovery_loss\": alpha_recovery_loss,\n",
    "            \"regularization_loss\": regularization_loss,\n",
    "            \"reconstructed_interpolated_images\": interpolated_images,\n",
    "            \"reconstructed_images\": reconstructed_images,\n",
    "            \"blended_non_interpolated_images\": regularization_images,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0822dc7-8dd6-4171-bbdd-610f6e539d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACAITrainer(Trainer):\n",
    "    autoencoder: ACAI\n",
    "    TRAIN_CRITIC_AFTER = 0\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        autoencoder: ACAI,\n",
    "        logger: Optional[WandbLogger] = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__(autoencoder, logger, device)\n",
    "        self.optimizer_ae = torch.optim.Adam([*autoencoder.encoder.parameters(), *autoencoder.decoder.parameters()], lr=3e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(autoencoder.critic.parameters(), lr=3e-4)\n",
    "        self._step_counter = 0\n",
    "    \n",
    "    def train_step(self):\n",
    "        self._train_ae()\n",
    "        if self.TRAIN_CRITIC_AFTER < self._step_counter:\n",
    "            self._train_critic()\n",
    "        self._step_counter += 1\n",
    "    \n",
    "    def _train_ae(self):\n",
    "        self.autoencoder.train()\n",
    "        self.autoencoder.critic.eval()\n",
    "        batch1 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch1[0])\n",
    "        out = self.autoencoder(batch1, batch2)\n",
    "        loss = out['loss']\n",
    "        self.optimizer_ae.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_ae.step()\n",
    "\n",
    "        self.logger.log({\"ae/train/loss\": loss.detach().item()})\n",
    "        self.logger.log({\"ae/train/reconstruction_loss\": out['reconstruction_loss'].detach().item()})\n",
    "        self.logger.log({\"ae/train/interpolation_loss\": out['interpolation_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/first_image_in_batch\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/ae/train/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        \n",
    "    def _train_critic(self):\n",
    "        self.autoencoder.eval()\n",
    "        self.autoencoder.critic.train()\n",
    "        batch1 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        batch2 = generate_batch(BATCHSIZE).to(self.device)\n",
    "        first_image = torch2numpy_image(batch1[0])\n",
    "        out = self.autoencoder.forward_critic(batch1, batch2)\n",
    "        loss = out['loss']\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        self.logger.log({\"critic/train/loss\": loss.detach().item()})\n",
    "        self.logger.log({\"critic/train/alpha_recovery_loss\": out['alpha_recovery_loss'].detach().item()})\n",
    "        self.logger.log({\"critic/train/regularization_loss\": out['regularization_loss'].detach().item()})\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/first_image_in_batch\", \n",
    "            collage_images([first_image, torch2numpy_image(out['reconstructed_images'][0])])\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/decoded_interpolation\",\n",
    "            torch2numpy_image(out[\"reconstructed_interpolated_images\"][0]),\n",
    "        )\n",
    "        self.logger.log_images(\n",
    "            \"images/critic/train/blended_image\",\n",
    "            torch2numpy_image(out[\"blended_non_interpolated_images\"][0]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f630b888-9a1f-44f4-8ed5-83fc1ef030e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mimplausible_deniability\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/maxim/acai/wandb/run-20231024_061633-297ffznm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/implausible_deniability/acai_karpathy/runs/297ffznm' target=\"_blank\">acai__lambda_divided_4</a></strong> to <a href='https://wandb.ai/implausible_deniability/acai_karpathy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/implausible_deniability/acai_karpathy' target=\"_blank\">https://wandb.ai/implausible_deniability/acai_karpathy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/implausible_deniability/acai_karpathy/runs/297ffznm' target=\"_blank\">https://wandb.ai/implausible_deniability/acai_karpathy/runs/297ffznm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mapply(init_weights_kaiming_normal)\n\u001b[1;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ACAITrainer(autoencoder, logger\u001b[38;5;241m=\u001b[39mlogger, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2600000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_steps)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEVAL_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_step(eval_batch)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mACAITrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_ae\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTRAIN_CRITIC_AFTER \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_counter:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_critic()\n",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m, in \u001b[0;36mACAITrainer._train_ae\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m batch2 \u001b[38;5;241m=\u001b[39m generate_batch(BATCHSIZE)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m first_image \u001b[38;5;241m=\u001b[39m torch2numpy_image(batch1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_ae\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m, in \u001b[0;36mACAI.forward\u001b[0;34m(self, images, images4interpolation)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: torch\u001b[38;5;241m.\u001b[39mTensor, images4interpolation: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 16\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     latent4interpolation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(images4interpolation)\n\u001b[1;32m     18\u001b[0m     interpolated_latent, alphas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mix_latents(latent, latent4interpolation)\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/cafa-protein/lib/python3.10/site-packages/torch/nn/modules/pooling.py:639\u001b[0m, in \u001b[0;36mAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_include_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdivisor_override\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fix_seeds(42)\n",
    "\n",
    "logger = WandbLogger(run='acai__lambda_divided_4', project='acai_karpathy', log_images_every=100)\n",
    "encoder = build_simple_encoder(width_coef=1)\n",
    "decoder = build_simple_decoder(width_coef=1)\n",
    "critic = build_simple_encoder(width_coef=1)\n",
    "autoencoder = ACAI(encoder, decoder, critic)\n",
    "autoencoder.apply(init_weights_kaiming_normal)\n",
    "\n",
    "trainer = ACAITrainer(autoencoder, logger=logger, device=torch.device(\"cuda\"))\n",
    "trainer.train(n_steps=2600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be674f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_encoder(encoder, decoder, idx=0):\n",
    "    N_STEPS = 9\n",
    "    images = generate_batch(2)\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    image1_latent, image2_latent = (encoder(image.unsqueeze(0)) for image in images)\n",
    "    interpolated_latents = [alpha*image1_latent + (1-alpha)*image2_latent for alpha in np.linspace(0.0, 1.0, N_STEPS)]\n",
    "    interpolated_images = [decoder(latent).squeeze() for latent in interpolated_latents]\n",
    "\n",
    "    fig, ax = plt.subplots(1, N_STEPS, figsize=(15, 5))\n",
    "    for i in range(N_STEPS):\n",
    "        ax[i].imshow(interpolated_images[i].detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784d3ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interpolate_encoder(autoencoder.encoder.cpu(), autoencoder.decoder.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c469bc-f422-4369-be73-1199d354c964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafa-protein",
   "language": "python",
   "name": "cafa-protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
